{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RCNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKSkt2AjZo1n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from torch.nn import functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilIkk_IdYW8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_size = 300\n",
        "hidden_layers = 1\n",
        "hidden_size = 64\n",
        "output_size = 4\n",
        "max_epochs = 15\n",
        "hidden_size_linear = 128\n",
        "lr = 0.5\n",
        "batch_size = 256\n",
        "dropout_rate = 0.7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc0jxp9kZ5Yv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RCNN(nn.Module):\n",
        "  def __init__(self, vocab_size, embeddings):\n",
        "    super(RCNN, self).__init__()\n",
        "\n",
        "    # embedding layer\n",
        "    self.embeddings = nn.Embedding(vocab_size,embedding_size)\n",
        "    self.embeddings.weight = nn.Parameter(embeddings, requires_grad=False)\n",
        "\n",
        "    #Use Bi-directional LSTM for RCNN instead of Vanilla RCNN\n",
        "    self.lstm == nn.LSTM(input_size = embedding_size,\n",
        "                         hidden_size = hidden_size,\n",
        "                         num_layers = hidden_layers,\n",
        "                         dropout = dropout_rate,\n",
        "                         birectional = True)\n",
        "    \n",
        "    self.dropout = nn.Droupout(dropout_rate)\n",
        "\n",
        "    self.W = nn.Linear(\n",
        "        embedding_size + 2*hidden_size,\n",
        "        hidden_size_linear\n",
        "    )\n",
        "\n",
        "    self.tanh = nn.Tanh()\n",
        "\n",
        "    self.fc = nn.Linear(\n",
        "        hidden_size_linear,\n",
        "        output_size\n",
        "    )\n",
        "\n",
        "    self.softmax = nn.Softmax()\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    embedded_sentence = self.embeddings(x)\n",
        "    lstm_out, (h_n, c_n) = self.lstm(embedded_sentence)\n",
        "    input_features = torch.cat([lstm_out,embedded_sentence],2).permute(1,0,2)\n",
        "\n",
        "    linear_out = self.tanh(\n",
        "        self.W(input_features)\n",
        "    )\n",
        "    linear_out = linear_out.permute(0,2,1)\n",
        "\n",
        "    max_out_features = F.max_pool1d(linear_output,linear_output.shape[2]).squeeze(2)\n",
        "    max_out_features = self.dropout(max_out_features)\n",
        "\n",
        "    final_out = self.fc(max_out_features)\n",
        "    return self.softmax(final_out)\n",
        "\n",
        "\n",
        "  def train(self, train_iterator, epoch):\n",
        "    losses = []\n",
        "\n",
        "    for i , batch in enumerate(train_iterator):\n",
        "      self.optimizer.zero_grad()\n",
        "      if torch.cuda.is_available():\n",
        "        x = batch.text.cuda()\n",
        "        y = (batch.label - 1).type(torch.cuda.LongTensor)\n",
        "      else:\n",
        "        x = batch.text\n",
        "        y = (batch.label - 1).type(torch.LongTensor)\n",
        "\n",
        "      y_pred = self.__call__(x)\n",
        "      loss = self.loss_op(y_pred,y)\n",
        "      loss.backward()\n",
        "      losses.append(loss.data.cpu().numpy())\n",
        "      self.optimizer.step()\n",
        "\n",
        "      if i % 100 == 0:\n",
        "        print(\"Iter: {}\".format(i+1))\n",
        "        avg_train_loss = np.mean(losses)\n",
        "        print(\"\\tAverage training loss: {:.4f}\".format(avg_train_loss))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}