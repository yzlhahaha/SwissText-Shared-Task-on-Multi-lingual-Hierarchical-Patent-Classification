Training/evaluation parameters Namespace(adam_epsilon=1e-08, arch='bert', do_data=False, do_lower_case=True, do_test=False, do_train=True, epochs=6, eval_batch_size=8, eval_max_seq_len=256, fp16=False, fp16_opt_level='O1', grad_clip=1.0, gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, loss_scale=0, mode='min', monitor='valid_loss', n_gpu='0', predict_checkpoints=0, resume_path='', save_best=True, seed=42, sorted=1, train_batch_size=8, train_max_seq_len=256, valid_size=0.2, warmup_proportion=0.1, weight_decay=0.01)
initializing model
loading configuration file /home/home1/xw176/work/Bert-Multi-Label-Text-Classification/pybert/pretrain/bert/base-uncased/config.json
Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 645,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

loading weights file /home/home1/xw176/work/Bert-Multi-Label-Text-Classification/pybert/pretrain/bert/base-uncased/pytorch_model.bin
Weights of BertForMultiLable not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
Weights from pretrained model not used in BertForMultiLable: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
initializing callbacks
***** Running training *****
  Num Epochs = 6
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 200000
Warning: There's no GPU available on this machine, training will be performed on CPU.
Warning: The number of GPU's configured to use is 0, but only 0 are available on this machine.
Epoch 1/6
Epoch 1 - summary 1/1746: summary_1779
Saving examples into cached file /home/home1/xw176/work/Bert-Multi-Label-Text-Classification/pybert/dataset/cached/cached_train_examples_1779_bert
*** Example ***
guid: train-0
tokens: [CLS] we ##g ##wer ##f ##hand ##sch ##uh zu ##m rein ##igen von tier ##en ; di ##sp ##osa ##ble glove for cleaning pets ; gan ##t jet ##able pour la ##ver des an ##ima ##ux ; a di ##sp ##osa ##ble glove for releasing a deter ##gent and / or anti - parasite formulation ##for pets and / or farm animals , which in contact with water generates foam which ##do ##es not need to be subsequently ri ##nsed . furthermore , the dry dorsal part ( 17 ' ) of the glove may be used to dry the animal after washing , and after ##cle ##ani ##ng , the glove is removed by being turned inside - out so that dirt and ##hair remain within . ; a di ##sp ##osa ##ble glove for releasing a deter ##gent and / or anti - parasite formulation ##for pets and / or farm animals , which in contact with water generates foam which ##do ##es not need to be subsequently ri ##nsed . furthermore , the dry dorsal part ( 17 ' ) of the glove may be used to dry the animal after washing , and after ##cle ##ani ##ng , the glove is removed by being turned inside - out so that dirt and ##hair remain within . ; * * field of the invention * * the present invention relates to a di ##sp ##osa ##ble glove for cleaning pets . * * background art * * the cleaning of pets and [SEP]
input_ids: 101 2057 2290 13777 2546 11774 11624 27225 16950 2213 27788 29206 3854 7563 2368 1025 4487 13102 8820 3468 15913 2005 9344 18551 1025 25957 2102 6892 3085 10364 2474 6299 4078 2019 9581 5602 1025 1037 4487 13102 8820 3468 15913 2005 8287 1037 28283 11461 1998 1013 2030 3424 1011 21198 20219 29278 18551 1998 1013 2030 3888 4176 1010 2029 1999 3967 2007 2300 19421 17952 2029 3527 2229 2025 2342 2000 2022 3525 15544 27730 1012 7297 1010 1996 4318 12759 2112 1006 2459 1005 1007 1997 1996 15913 2089 2022 2109 2000 4318 1996 4111 2044 12699 1010 1998 2044 14321 7088 3070 1010 1996 15913 2003 3718 2011 2108 2357 2503 1011 2041 2061 2008 6900 1998 26227 3961 2306 1012 1025 1037 4487 13102 8820 3468 15913 2005 8287 1037 28283 11461 1998 1013 2030 3424 1011 21198 20219 29278 18551 1998 1013 2030 3888 4176 1010 2029 1999 3967 2007 2300 19421 17952 2029 3527 2229 2025 2342 2000 2022 3525 15544 27730 1012 7297 1010 1996 4318 12759 2112 1006 2459 1005 1007 1997 1996 15913 2089 2022 2109 2000 4318 1996 4111 2044 12699 1010 1998 2044 14321 7088 3070 1010 1996 15913 2003 3718 2011 2108 2357 2503 1011 2041 2061 2008 6900 1998 26227 3961 2306 1012 1025 1008 1008 2492 1997 1996 11028 1008 1008 1996 2556 11028 14623 2000 1037 4487 13102 8820 3468 15913 2005 9344 18551 1012 1008 1008 4281 2396 1008 1008 1996 9344 1997 18551 1998 102
input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
*** Example ***
guid: train-1
tokens: [CLS] ve ##rf ##ah ##ren zu ##m hers ##tell ##en eine ##r rotor ##sch ##au ##fe ##l eine ##r wind ##ener ##gie ##an ##lage ; methods of making wind turbine rotor blades ; pro ##cede de fabrication des au ##bes pour e ##oli ##enne ; a method of manufacturing a wind turbine rotor blade ( 114 ) includes , in one ##em ##bo ##diment , the steps of providing a core ( 120 ) , and applying at least one ##re ##in ##for ##cing skin ( 126 ) to the core to form a blade sub ##asse ##mb ##ly ( 131 ) . each ##re ##in ##for ##cing skin is formed from a mat of rein ##for ##cing fibers . the method also ##in ##cl ##udes applying a micro - por ##ous membrane ( 128 ) over the at least one ##re ##in ##for ##cing skin , applying a vacuum film ( 138 ) over the micro - por ##ous membrane , introducing a polymer ##ic resin to the core , in ##fus ##ing the resin through the core ##and through the at least one rein ##for ##cing skin by applying a vacuum to the ##bla ##de assembly , and cu ##ring the resin to form the rotor blade . ; a method of manufacturing a wind turbine rotor blade ( 114 ) includes , in one ##em ##bo ##diment , the steps of providing a core ( 120 ) , and applying at least one ##re ##in ##for ##cing skin ( 126 ) to [SEP]
input_ids: 101 2310 12881 4430 7389 16950 2213 5106 23567 2368 27665 2099 18929 11624 4887 7959 2140 27665 2099 3612 24454 11239 2319 20679 1025 4725 1997 2437 3612 14027 18929 10491 1025 4013 22119 2139 25884 4078 8740 12681 10364 1041 10893 24336 1025 1037 4118 1997 5814 1037 3612 14027 18929 6085 1006 12457 1007 2950 1010 1999 2028 6633 5092 21341 1010 1996 4084 1997 4346 1037 4563 1006 6036 1007 1010 1998 11243 2012 2560 2028 2890 2378 29278 6129 3096 1006 14010 1007 2000 1996 4563 2000 2433 1037 6085 4942 27241 14905 2135 1006 14677 1007 1012 2169 2890 2378 29278 6129 3096 2003 2719 2013 1037 13523 1997 27788 29278 6129 16662 1012 1996 4118 2036 2378 20464 22087 11243 1037 12702 1011 18499 3560 10804 1006 11899 1007 2058 1996 2012 2560 2028 2890 2378 29278 6129 3096 1010 11243 1037 11641 2143 1006 15028 1007 2058 1996 12702 1011 18499 3560 10804 1010 10449 1037 17782 2594 24604 2000 1996 4563 1010 1999 25608 2075 1996 24604 2083 1996 4563 5685 2083 1996 2012 2560 2028 27788 29278 6129 3096 2011 11243 1037 11641 2000 1996 28522 3207 3320 1010 1998 12731 4892 1996 24604 2000 2433 1996 18929 6085 1012 1025 1037 4118 1997 5814 1037 3612 14027 18929 6085 1006 12457 1007 2950 1010 1999 2028 6633 5092 21341 1010 1996 4084 1997 4346 1037 4563 1006 6036 1007 1010 1998 11243 2012 2560 2028 2890 2378 29278 6129 3096 1006 14010 1007 2000 102
input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
Saving features into cached file /home/home1/xw176/work/Bert-Multi-Label-Text-Classification/pybert/dataset/cached/cached_train_features_1779_256_bert
sorted data by th length of input
Loading examples from cached file /home/home1/xw176/work/Bert-Multi-Label-Text-Classification/pybert/dataset/summary_pickle/cached_valid_examples_1779_bert
Loading features from cached file /home/home1/xw176/work/Bert-Multi-Label-Text-Classification/pybert/dataset/summary_pickle/cached_valid_features_1779_256_bert
Epoch 1 - summary 2/1746: summary_2423
Saving examples into cached file /home/home1/xw176/work/Bert-Multi-Label-Text-Classification/pybert/dataset/cached/cached_train_examples_2423_bert
*** Example ***
guid: train-0
tokens: [CLS] ve ##rf ##ah ##ren und vo ##rri ##cht ##ung zur ra ##uch ##gas ##be ##hand ##lun ##g ; method and apparatus for flu ##e gas treatment ; pro ##cede et app ##are ##il pour le trait ##ement de ga ##z comb ##ust ##ible ; a method for flu ##e gas treatment includes causing a combustion in a boiler ( 15 , 19 ) using at least a part of a flu ##e gas ( 12 ) emitted from a gas turbine ( 11 ) and introduced from at least one of an upstream side and a downstream side of ##an exhaust heat recovery boiler ( 13 ) , which recover ##s a high - temperature heat ##of the flu ##e gas ( 12 ) , so as to increase a concentration of carbon dioxide int ##he flu ##e gas , and recovering carbon dioxide in a carbon dioxide recovery ##app ##arat ##us ( 18 ) . ; a method for flu ##e gas treatment includes causing a combustion in a boiler ( 15 , 19 ) using at least a part of a flu ##e gas ( 12 ) emitted from a gas turbine ( 11 ) and introduced from at least one of an upstream side and a downstream side of ##an exhaust heat recovery boiler ( 13 ) , which recover ##s a high - temperature heat ##of the flu ##e gas ( 12 ) , so as to increase a concentration of carbon dioxide int ##he flu ##e gas , and [SEP]
input_ids: 101 2310 12881 4430 7389 6151 29536 18752 10143 5575 17924 10958 10875 12617 4783 11774 26896 2290 1025 4118 1998 14709 2005 19857 2063 3806 3949 1025 4013 22119 3802 10439 12069 4014 10364 3393 18275 13665 2139 11721 2480 22863 19966 7028 1025 1037 4118 2005 19857 2063 3806 3949 2950 4786 1037 16513 1999 1037 15635 1006 2321 1010 2539 1007 2478 2012 2560 1037 2112 1997 1037 19857 2063 3806 1006 2260 1007 22627 2013 1037 3806 14027 1006 2340 1007 1998 3107 2013 2012 2560 2028 1997 2019 13909 2217 1998 1037 13248 2217 1997 2319 15095 3684 7233 15635 1006 2410 1007 1010 2029 8980 2015 1037 2152 1011 4860 3684 11253 1996 19857 2063 3806 1006 2260 1007 1010 2061 2004 2000 3623 1037 6693 1997 6351 14384 20014 5369 19857 2063 3806 1010 1998 13400 6351 14384 1999 1037 6351 14384 7233 29098 25879 2271 1006 2324 1007 1012 1025 1037 4118 2005 19857 2063 3806 3949 2950 4786 1037 16513 1999 1037 15635 1006 2321 1010 2539 1007 2478 2012 2560 1037 2112 1997 1037 19857 2063 3806 1006 2260 1007 22627 2013 1037 3806 14027 1006 2340 1007 1998 3107 2013 2012 2560 2028 1997 2019 13909 2217 1998 1037 13248 2217 1997 2319 15095 3684 7233 15635 1006 2410 1007 1010 2029 8980 2015 1037 2152 1011 4860 3684 11253 1996 19857 2063 3806 1006 2260 1007 1010 2061 2004 2000 3623 1037 6693 1997 6351 14384 20014 5369 19857 2063 3806 1010 1998 102
input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
*** Example ***
guid: train-1
tokens: [CLS] ve ##rf ##ah ##ren zur ru ##ck ##ge ##win ##nu ##ng von en ##er ##gie aus dem ab ##gas eine ##s br ##enne ##rs ; method for regaining energy from the exhaust gas of a burn ##er ; pro ##cede de rec ##up ##eration d ' en ##er ##gie a part ##ir du ga ##z d ' ec ##ha ##ppe ##ment d ' un br ##ule ##ur ; zur damp ##fer ##ze ##ug ##ung we ##rden br ##enne ##r ( 11 ) ein ##ges ##etz ##t , die eine ##n damp ##fk ##ess ##el ( 10 ) be ##hei ##zen . der br ##enne ##r ( 11 ) er ##ze ##ug ##t ab ##gas mit eine ##r ve ##rh ##al ##t ##nism ##a ##ß ##ig ho ##hen ##tem ##per ##at ##ur . es ist be ##kan ##nt , eine ##n te ##il der en ##er ##gie im he ##i ##ße ##n ab ##gas des ##bre ##nne ##rs ( 11 ) zur ##uck ##zu ##ge ##win ##nen du ##rch vo ##r ##war ##mun ##g der dem br ##enne ##r ( 11 ) zu ##zu ##fu ##hre ##nden verb ##ren ##nu ##ng ##sl ##uf ##t . dana ##ch hat das ab ##gas im ##mer no ##ch eine ##ver ##hal ##t ##nism ##a ##ß ##ig ho ##he temper ##at ##ur von bis zu 120 ##° ##c . die er ##fin ##dun ##g si ##eh ##t es vo ##r , dem ab ##gas des br ##enne ##rs ( 11 ) zu ##sat ##z ##liche en ##er ##gie ##zu [SEP]
input_ids: 101 2310 12881 4430 7389 17924 21766 3600 3351 10105 11231 3070 3854 4372 2121 11239 17151 17183 11113 12617 27665 2015 7987 24336 2869 1025 4118 2005 28657 2943 2013 1996 15095 3806 1997 1037 6402 2121 1025 4013 22119 2139 28667 6279 16754 1040 1005 4372 2121 11239 1037 2112 4313 4241 11721 2480 1040 1005 14925 3270 21512 3672 1040 1005 4895 7987 9307 3126 1025 17924 10620 7512 4371 15916 5575 2057 18246 7987 24336 2099 1006 2340 1007 16417 8449 26327 2102 1010 3280 27665 2078 10620 24316 7971 2884 1006 2184 1007 2022 26036 10431 1012 4315 7987 24336 2099 1006 2340 1007 9413 4371 15916 2102 11113 12617 10210 27665 2099 2310 25032 2389 2102 28113 2050 19310 8004 7570 10222 18532 4842 4017 3126 1012 9686 21541 2022 9126 3372 1010 27665 2078 8915 4014 4315 4372 2121 11239 10047 2002 2072 17499 2078 11113 12617 4078 13578 10087 2869 1006 2340 1007 17924 12722 9759 3351 10105 10224 4241 11140 29536 2099 9028 23041 2290 4315 17183 7987 24336 2099 1006 2340 1007 16950 9759 11263 28362 25915 12034 7389 11231 3070 14540 16093 2102 1012 11271 2818 6045 8695 11113 12617 10047 5017 2053 2818 27665 6299 8865 2102 28113 2050 19310 8004 7570 5369 12178 4017 3126 3854 20377 16950 6036 7737 2278 1012 3280 9413 16294 27584 2290 9033 11106 2102 9686 29536 2099 1010 17183 11113 12617 4078 7987 24336 2869 1006 2340 1007 16950 16846 2480 27412 4372 2121 11239 9759 102
input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
